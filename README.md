# Malware-Classification-using-API-Calls
![image](https://user-images.githubusercontent.com/95842589/209646638-dae161a8-3acd-4c9f-b2f4-8544b34fb61a.png)
## Problem definition 
- There are many known malware families, we need to distinguish between them.
- Members from same family show similar behavior.

## Methodology
A. Dataset
    Our dataset contains of 1000 calls and 1000 types which are translated the families produced by each of the
  software into 8 main malware families: Trojan, Backdoor, Downloader, Worms, Spyware Adware, Dropper, 
  Virus. Table 1 shows the number of malwares belonging to malware families in our data set. As you can see 
  in the table, the number of samples of other malware families except AdWare is quite close to each other. 
  There is such a difference because we don't find too much of malware from the adware malware family.
    
![image](https://user-images.githubusercontent.com/95842589/209646883-ed6e7942-07da-4b7b-9ba2-64ba7edefa4e.png)

B. Modeling
 There is such a difference because we don't find too much of malware from the adware malware family. The 
purposed techniques are used to classify the 8 families of the malware. Different machine learning algorithms, 
such as Logistic Regression, Decision trees, Support vector Machine, Hist gradient boosting, Random forest 
and KNN are compared to see which one be best suit the situation and can be used on our data efficiently. 
These are the steps for selecting the best model proceed.
1. Read the dataset
2. Preprocessing on the data by checking for nulls then splitting sequences and adding padding then 
applying the label encoder and splitting the data into training and testing sets by train test split with 
providing 80% of the data for training and 20% testing.
3. Performing Exploratory Data Analysis (EDA) which is an approach to analyze the data using 
visual techniques. It is used to discover trends, patterns, or to check assumptions with the help of 
statistical summary and graphical representations. It allows a machine learning model to predict our 
dataset better. Gives you more accurate results. It also helps us to choose a better machine learning 
model.
  
  3.1. First, Showing the distribution of the labels.

![image](https://user-images.githubusercontent.com/95842589/209647154-6ad88161-e84e-4223-b0ad-f9ace56fad69.png)

  3.2. Applying dimensionality reduction and choosing baseline model which was the Logistic Regression. It achieved 30% accuracy.
  
  ![image](https://user-images.githubusercontent.com/95842589/209647253-96c89afc-be8b-44a1-ba3c-17bc70a95573.png)

    
   3.3. Applying PCA (Principal component Analysis) to enable us to visualize multidimensional data 
        and reduce the dimensionality (number of features) within a dataset while still retaining as much 
        information as possible. It helped us to produce variety of the accuracy to our model after applying 
        it. The accuracy after applying PCA on data was 30.4%. This figure shows number of features and 
        accuracy.

![image](https://user-images.githubusercontent.com/95842589/209647370-1b40676f-d744-4d2b-b0c9-8e557666f106.png)


4. Models' Training Step
    4.1. Appling multiple models to try on the data which are LR, DT, SVM, HG, RF and KNN
          Logistic Regression (LR): is a classification algorithm. It is used to predict a binary outcome 
          based on a set of independent variables.
          Decision Tree (DT): is the most powerful and popular tool for classification and prediction. A 
          Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an 
          attribute, each branch represents an outcome of the test, and each leaf node (terminal node) 
          holds a class label.
          Support Vector Machine (SVM): is a supervised machine learning algorithm used for both 
          classification and regression. Though we say regression problems as well its best suited for 
          classification. The objective of SVM algorithm is to find a hyperplane in an N-dimensional 
          space that distinctly classifies the data points. The dimension of the hyperplane depends upon 
          the number of features.
          Hist Gradient Boosting (HG): is an ensemble machine learning algorithm. Boosting refers to 
          a class of ensemble learning algorithms that add tree models to an ensemble sequentially [10].
          Random Forest (RF): a meta estimator that fits a number of decision tree classifiers on various 
          sub-samples of the dataset and uses averaging to improve the predictive accuracy and control 
          over-fitting .
          K-Nearest Neighbor (KNN): is a type of supervised machine learning algorithm used to solve 
          classification and regression problems. However, it's mainly used for classification problems.
          
   4.2. Then evaluating each model in turn and applying Kfold cross-validator which split dataset into k 
        consecutive folds (without shuffling by default). Each fold is used once as a validation while the k-1 
        remaining folds form the training set. Read more in the User Guide.
        
   4.3. Comparing the accuracies of the models and choosing the highest two at least which were Random 
        Forest Classifier and Hist Gradient Boosting Classifier.
        
   4.4. Applying Grid search to find the best hyperparameter of Random Forest model then extracted the 
        best model which was Hist Gradient Boosting Classifier and best parameters.

5. Evaluation Step: Making an evaluation for the champion model which was Hist Gradient Boosting. 
  It achieved the highest accuracy which is 41.2%.


## Conclusion
  This paper introduces a malware classification system using six different machine learning models based on a public 
  malware dataset generated by Cuckoo Sandbox. After training and testing our six models, we have found that the Hist 
  Gradient model outperformed the other models with an average test accuracy of 47%. Fortunately, the Adware 
  malware family have been correctly classified with a testing accuracy of 98%. Yet unfortunately, almost half of the 
  malware samples could not be classified correctly, this is because those malware variants show similar features with 
  other malware types. For the future work, we aim to propose a more precise detection system using deep learning 
  which specifically detects and classifies all malware variants.
